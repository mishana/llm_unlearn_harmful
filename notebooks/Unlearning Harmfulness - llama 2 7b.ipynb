from transformers import AutoTokenizer, pipeline, BitsAndBytesConfig, AutoModelForCausalLM
import torch

original_model_name = "meta-llama/Llama-2-7b-hf"
unlearned_model_name = "odnurih/Llama-2-7b-hf-unlearn-harm"

tokenizer = AutoTokenizer.from_pretrained(original_model_name)

orig_model = AutoModelForCausalLM.from_pretrained(original_model_name, device_map="auto")
orig_generator = pipeline('text-generation', model=orig_model, tokenizer=tokenizer, max_new_tokens=50)
